seed: [0] #[1,2,3,4]
device: 'cuda'
name: 'TRIAL_Averaging' # use 4digit name tag to overwrite experiment with same parameters
docker: True
simulation: False 
port: 8080 
# Ports to use: 8080, 24144, 48487, 45513, 997857509
# test_unseen: False # this will turn off train mode and test on a specified center
strategy:
    # min_available_clients: 2 # Change to increase the number of expected clients
    # min_fit_clients: 2
    # min_eval_clients: 2
    CD_P: 1.0 # Center Dropout [Percentage]. 1 means no dropout. 
    aggregator: aggregator.FedAvg #Smooth
    smoothing: 1 # None, or float in [0,1]
    # Note that if CD_P results in a lower number of clients than min_fit_clients, algorithm will default to min_fit_clients. Make sure to adjust accordingly
hyperparameters:
    federated_rounds: 120 # Number of federated rounds to run. This is the number of times the clients will send their data to the server.
    epochs_per_round : 1 
    epochs_last_round: 10 # To test the fine-tuning.
    epochs_strategy: False #"dynamic_epochs"
    batch_size : 10  # if 0, it will adapt to center
    test_batch_size : 10 #use a higher one for speed up if CUDA memory allows  
    lr : 0.01
    early_stop_counter: 20 # number of iterations after performance stagnation before stopping
    ## MULTI VS BINARY CLASSIFICATION
    criterion : torch.nn.BCELoss
    # criterion : torch.nn.CrossEntropyLoss
    optimizer : torch.optim.SGD
model:
    # The model name used here will generate a new experiment in the ~/mnm/experiments
    arch:
        function: models.nets.ResNet50Classifier
        args: #keyword arguments for function
            pretrained: True
            in_ch: 1
            out_ch: 1
            early_layers_learning_rate: 1e-5 #10^-5, if set to 0 early layers will be frozen 
            seed: 42 # Particularly important for federated. Models will make no sense if we aggregate from different initializations
    pretrained: # if we have our own weights
        isTrue: False
data:
    pathologies: ['mass'] #,'RV','DCM','MINF'] #, 'RV'] #, 'DCM' ['DCM', 'HCM', 'NOR', 'RV']
    labels: [0,1]
    load_max: -1 # -1 means load all
    balance: True
    rotate: False
    flip: True
paths:
    # Used by client:
    # These are files we send the clients. So the path we define ourselves:
    #landmarks: /home/akis-linardos/BFP/src/preprocessing/optimam_train_hologic_landmarks.pth # out of docker
    landmarks: /BFP/src/preprocessing/optimam_train_hologic_landmarks.pth # in docker
    # ub_logs: workenv/BFP/src/visualizations # Ignore this
    # These are paths we should be given from the partners. The docker should mount them:
    csv_path: /home/lidia-garrucho/datasets/OPTIMAM/png_screening_cropped_fixed/client_images_screening.csv
    dataset_path: /home/lidia-garrucho/datasets/OPTIMAM/png_screening_cropped_fixed/images
    # Used by server:
    # logs: BFP/src/server_logs
    experiments: BFP/src/experiments
    misc: experiment/misc
    continue_from_checkpoint: False #experiments/0193_FAvg4EPR_ResNet50_LateralityFixThenFlipAgain #False #/experiments/experiment_folder # If true change False with path to current experiment folder (Not path to checkpoint)